# Exercise 5

In this exercise, we continue with [20
newsgroups](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html)
text categorization using and finetuning a pretrained BERT model.

## Files

### TF2/Keras

* *tf2-20ng-bert.py*: 20 newsgroups classification with BERT finetuning

### PyTorch

* *pytorch_20ng_bert.py*: 20 newsgroups classification with BERT finetuning

## Tasks

### Task 1

Run the models and compare its accuracy and runtime to the RNN and CNN
models of [Exercise 3](Exercise_3.md).

### Task 2

Try to improve the results, e.g., by tweaking the model or the training parameters (optimizer, batch size, number of epochs, etc.). You can also work on replacing BERT with another Transformers model; see [documentation](https://huggingface.co/transformers/).
